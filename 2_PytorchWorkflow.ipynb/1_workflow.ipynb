{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  torch.Size([100000, 1]) Shape of Y:  torch.Size([100000, 1])\n",
      "tensor([0.]) tensor([2.])\n",
      "tensor([0.0010]) tensor([2.0006])\n",
      "tensor([0.0020]) tensor([2.0012])\n",
      "tensor([0.0030]) tensor([2.0018])\n",
      "tensor([0.0040]) tensor([2.0024])\n",
      "tensor([0.0050]) tensor([2.0030])\n",
      "tensor([0.0060]) tensor([2.0036])\n",
      "tensor([0.0070]) tensor([2.0042])\n",
      "tensor([0.0080]) tensor([2.0048])\n",
      "tensor([0.0090]) tensor([2.0054])\n"
     ]
    }
   ],
   "source": [
    "weight = 0.6\n",
    "bias = 2\n",
    "\n",
    "X = torch.arange(start=0, end=100, step=0.001).unsqueeze(dim=1)\n",
    "Y = weight * X + bias\n",
    "\n",
    "print(\"Shape of X: \", X.shape,\"Shape of Y: \",  Y.shape, )\n",
    "\n",
    "for i in range(len(X[:10])):\n",
    "    print(X[i], Y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size:  85000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([85000, 1]), torch.Size([15000, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = int(0.85 * len(X))\n",
    "print(\"Training Size: \", train_len)\n",
    "\n",
    "train_X, train_y = X[:train_len], Y[:train_len]\n",
    "test_X, test_Y = X[train_len:], Y[train_len:]\n",
    "\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotData(X, Y, c = 'b'):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.scatter(X, Y, s = 3, c = c )\n",
    "    plt.show()\n",
    "\n",
    "plotData(train_X, train_y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x:torch.tensor) -> torch.tensor:\n",
    "        return self.weights * x + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  [Parameter containing:\n",
      "tensor([0.3367], requires_grad=True), Parameter containing:\n",
      "tensor([0.1288], requires_grad=True)]\n",
      "State Dict:  OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_test = LinearRegression()\n",
    "\n",
    "print(\"Parameters: \", list(model_test.parameters()))\n",
    "print(\"State Dict: \", model_test.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[28.7475],\n",
       "         [28.7478],\n",
       "         [28.7482],\n",
       "         [28.7485],\n",
       "         [28.7488],\n",
       "         [28.7492],\n",
       "         [28.7495],\n",
       "         [28.7498],\n",
       "         [28.7502],\n",
       "         [28.7505]]),\n",
       " tensor([[53.0000],\n",
       "         [53.0006],\n",
       "         [53.0012],\n",
       "         [53.0018],\n",
       "         [53.0024],\n",
       "         [53.0030],\n",
       "         [53.0036],\n",
       "         [53.0042],\n",
       "         [53.0048],\n",
       "         [53.0054]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting value without training the model\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_pred = model_test(test_X)\n",
    "\n",
    "# We can do the same this using\n",
    "# with torch.no_grad():\n",
    "#     y_pred = model_test(test_X)\n",
    "\n",
    "y_pred[:10], test_Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, Train loss is 13.061718940734863 ans test loss is 13.094616889953613\n",
      "[OrderedDict([('weights', tensor([0.7617])), ('bias', tensor([0.1388]))])]\n",
      " Epoch: 20, Train loss is 9.004914283752441 ans test loss is 21.899574279785156\n",
      "[OrderedDict([('weights', tensor([0.8566])), ('bias', tensor([0.1597]))])]\n",
      " Epoch: 40, Train loss is 6.842517852783203 ans test loss is 26.587886810302734\n",
      "[OrderedDict([('weights', tensor([0.9072])), ('bias', tensor([0.1751]))])]\n",
      " Epoch: 60, Train loss is 5.253997325897217 ans test loss is 30.029743194580078\n",
      "[OrderedDict([('weights', tensor([0.9442])), ('bias', tensor([0.1883]))])]\n",
      " Epoch: 80, Train loss is 3.9681782722473145 ans test loss is 32.814334869384766\n",
      "[OrderedDict([('weights', tensor([0.9742])), ('bias', tensor([0.2002]))])]\n",
      " Epoch: 100, Train loss is 2.8747856616973877 ans test loss is 35.18120574951172\n",
      "[OrderedDict([('weights', tensor([0.9997])), ('bias', tensor([0.2112]))])]\n",
      " Epoch: 120, Train loss is 1.9165793657302856 ans test loss is 37.25468063354492\n",
      "[OrderedDict([('weights', tensor([1.0220])), ('bias', tensor([0.2214]))])]\n",
      " Epoch: 140, Train loss is 1.0596033334732056 ans test loss is 39.10847091674805\n",
      "[OrderedDict([('weights', tensor([1.0419])), ('bias', tensor([0.2311]))])]\n",
      " Epoch: 160, Train loss is 4.901642799377441 ans test loss is 30.73464012145996\n",
      "[OrderedDict([('weights', tensor([0.9513])), ('bias', tensor([0.2410]))])]\n",
      " Epoch: 180, Train loss is 3.727576494216919 ans test loss is 33.276634216308594\n",
      "[OrderedDict([('weights', tensor([0.9786])), ('bias', tensor([0.2524]))])]\n",
      " Epoch: 200, Train loss is 2.7150650024414062 ans test loss is 35.46794891357422\n",
      "[OrderedDict([('weights', tensor([1.0022])), ('bias', tensor([0.2629]))])]\n",
      " Epoch: 220, Train loss is 1.8193738460540771 ans test loss is 37.405757904052734\n",
      "[OrderedDict([('weights', tensor([1.0231])), ('bias', tensor([0.2728]))])]\n",
      " Epoch: 240, Train loss is 1.0129865407943726 ans test loss is 39.149803161621094\n",
      "[OrderedDict([('weights', tensor([1.0418])), ('bias', tensor([0.2822]))])]\n",
      " Epoch: 260, Train loss is 3.0660204887390137 ans test loss is 34.67087173461914\n",
      "[OrderedDict([('weights', tensor([0.9933])), ('bias', tensor([0.2912]))])]\n",
      " Epoch: 280, Train loss is 2.161069393157959 ans test loss is 36.628780364990234\n",
      "[OrderedDict([('weights', tensor([1.0144])), ('bias', tensor([0.3011]))])]\n",
      " Epoch: 300, Train loss is 1.3487586975097656 ans test loss is 38.38565444946289\n",
      "[OrderedDict([('weights', tensor([1.0332])), ('bias', tensor([0.3105]))])]\n",
      " Epoch: 320, Train loss is 2.375152111053467 ans test loss is 36.14142990112305\n",
      "[OrderedDict([('weights', tensor([1.0089])), ('bias', tensor([0.3193]))])]\n",
      " Epoch: 340, Train loss is 1.559401273727417 ans test loss is 37.90578079223633\n",
      "[OrderedDict([('weights', tensor([1.0279])), ('bias', tensor([0.3287]))])]\n",
      " Epoch: 360, Train loss is 0.8188961148262024 ans test loss is 36.231510162353516\n",
      "[OrderedDict([('weights', tensor([1.0097])), ('bias', tensor([0.3373]))])]\n",
      " Epoch: 380, Train loss is 1.5277395248413086 ans test loss is 37.95360565185547\n",
      "[OrderedDict([('weights', tensor([1.0282])), ('bias', tensor([0.3467]))])]\n",
      " Epoch: 400, Train loss is 0.8041056990623474 ans test loss is 35.05707550048828\n",
      "[OrderedDict([('weights', tensor([0.9968])), ('bias', tensor([0.3550]))])]\n",
      " Epoch: 420, Train loss is 2.0260252952575684 ans test loss is 36.84810256958008\n",
      "[OrderedDict([('weights', tensor([1.0160])), ('bias', tensor([0.3645]))])]\n",
      " Epoch: 440, Train loss is 1.2764896154403687 ans test loss is 38.46880340576172\n",
      "[OrderedDict([('weights', tensor([1.0335])), ('bias', tensor([0.3736]))])]\n",
      " Epoch: 460, Train loss is 2.6627278327941895 ans test loss is 35.441802978515625\n",
      "[OrderedDict([('weights', tensor([1.0006])), ('bias', tensor([0.3820]))])]\n",
      " Epoch: 480, Train loss is 1.8747870922088623 ans test loss is 37.14582443237305\n",
      "[OrderedDict([('weights', tensor([1.0190])), ('bias', tensor([0.3912]))])]\n",
      " Epoch: 500, Train loss is 1.158085823059082 ans test loss is 38.69529724121094\n",
      "[OrderedDict([('weights', tensor([1.0356])), ('bias', tensor([0.4001]))])]\n",
      " Epoch: 520, Train loss is 4.161066055297852 ans test loss is 32.14966583251953\n",
      "[OrderedDict([('weights', tensor([0.9648])), ('bias', tensor([0.4083]))])]\n",
      " Epoch: 540, Train loss is 3.252277135848999 ans test loss is 34.11589813232422\n",
      "[OrderedDict([('weights', tensor([0.9859])), ('bias', tensor([0.4183]))])]\n",
      " Epoch: 560, Train loss is 2.4431381225585938 ans test loss is 35.86589813232422\n",
      "[OrderedDict([('weights', tensor([1.0047])), ('bias', tensor([0.4277]))])]\n",
      " Epoch: 580, Train loss is 1.7112315893173218 ans test loss is 37.448368072509766\n",
      "[OrderedDict([('weights', tensor([1.0218])), ('bias', tensor([0.4367]))])]\n",
      " Epoch: 600, Train loss is 1.0413172245025635 ans test loss is 38.8963508605957\n",
      "[OrderedDict([('weights', tensor([1.0373])), ('bias', tensor([0.4452]))])]\n",
      " Epoch: 620, Train loss is 4.758310794830322 ans test loss is 30.796754837036133\n",
      "[OrderedDict([('weights', tensor([0.9497])), ('bias', tensor([0.4534]))])]\n",
      " Epoch: 640, Train loss is 3.8280844688415527 ans test loss is 32.809505462646484\n",
      "[OrderedDict([('weights', tensor([0.9713])), ('bias', tensor([0.4635]))])]\n",
      " Epoch: 660, Train loss is 3.005553722381592 ans test loss is 34.58856964111328\n",
      "[OrderedDict([('weights', tensor([0.9904])), ('bias', tensor([0.4730]))])]\n",
      " Epoch: 680, Train loss is 2.2652392387390137 ans test loss is 36.18926239013672\n",
      "[OrderedDict([('weights', tensor([1.0076])), ('bias', tensor([0.4820]))])]\n",
      " Epoch: 700, Train loss is 1.5902457237243652 ans test loss is 37.64826965332031\n",
      "[OrderedDict([('weights', tensor([1.0233])), ('bias', tensor([0.4906]))])]\n",
      " Epoch: 720, Train loss is 0.9684544205665588 ans test loss is 38.99189758300781\n",
      "[OrderedDict([('weights', tensor([1.0378])), ('bias', tensor([0.4988]))])]\n",
      " Epoch: 740, Train loss is 2.2752761840820312 ans test loss is 36.138282775878906\n",
      "[OrderedDict([('weights', tensor([1.0068])), ('bias', tensor([0.5067]))])]\n",
      " Epoch: 760, Train loss is 1.6186777353286743 ans test loss is 37.557395935058594\n",
      "[OrderedDict([('weights', tensor([1.0221])), ('bias', tensor([0.5152]))])]\n",
      " Epoch: 780, Train loss is 1.0130010843276978 ans test loss is 38.86607360839844\n",
      "[OrderedDict([('weights', tensor([1.0361])), ('bias', tensor([0.5233]))])]\n",
      " Epoch: 800, Train loss is 1.8068753480911255 ans test loss is 37.12910079956055\n",
      "[OrderedDict([('weights', tensor([1.0173])), ('bias', tensor([0.5311]))])]\n",
      " Epoch: 820, Train loss is 1.200609564781189 ans test loss is 38.439048767089844\n",
      "[OrderedDict([('weights', tensor([1.0314])), ('bias', tensor([0.5392]))])]\n",
      " Epoch: 840, Train loss is 4.242831707000732 ans test loss is 31.809467315673828\n",
      "[OrderedDict([('weights', tensor([0.9596])), ('bias', tensor([0.5462]))])]\n",
      " Epoch: 860, Train loss is 3.457197904586792 ans test loss is 33.508480072021484\n",
      "[OrderedDict([('weights', tensor([0.9779])), ('bias', tensor([0.5555]))])]\n",
      " Epoch: 880, Train loss is 2.7495808601379395 ans test loss is 35.038246154785156\n",
      "[OrderedDict([('weights', tensor([0.9943])), ('bias', tensor([0.5643]))])]\n",
      " Epoch: 900, Train loss is 2.10391902923584 ans test loss is 36.433631896972656\n",
      "[OrderedDict([('weights', tensor([1.0093])), ('bias', tensor([0.5727]))])]\n",
      " Epoch: 920, Train loss is 1.5089592933654785 ans test loss is 37.719078063964844\n",
      "[OrderedDict([('weights', tensor([1.0231])), ('bias', tensor([0.5808]))])]\n",
      " Epoch: 940, Train loss is 0.9564826488494873 ans test loss is 38.912391662597656\n",
      "[OrderedDict([('weights', tensor([1.0359])), ('bias', tensor([0.5885]))])]\n",
      " Epoch: 960, Train loss is 1.855809211730957 ans test loss is 36.946311950683594\n",
      "[OrderedDict([('weights', tensor([1.0146])), ('bias', tensor([0.5959]))])]\n",
      " Epoch: 980, Train loss is 1.2933716773986816 ans test loss is 38.16122817993164\n",
      "[OrderedDict([('weights', tensor([1.0277])), ('bias', tensor([0.6038]))])]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "lossFn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model_test.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    model_test.train()\n",
    "\n",
    "    y_pred = model_test(train_X)\n",
    "\n",
    "    loss = lossFn(y_pred, train_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model_test.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_pred = model_test(test_X)\n",
    "        test_loss = lossFn(y_pred, test_Y)\n",
    "    if each_epoch % 20 == 0:\n",
    "        print(f\" Epoch: {each_epoch}, Train loss is {loss.item()} ans test loss is {test_loss.item()}\")\n",
    "        print([model_test.state_dict()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OrderedDict([('weights', tensor([0.6148])), ('bias', tensor([0.6013]))])]\n"
     ]
    }
   ],
   "source": [
    "print([model_test.state_dict()])\n",
    "# with torch.no_grad():\n",
    "#     y_pred = model_test(test_Y)\n",
    "\n",
    "# y_pred[:10], test_Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Model\n",
    "\n",
    "torch.save() \n",
    "\n",
    "torch.load() \n",
    "\n",
    "torch.nn.Module.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "modelPath = Path(\"model.pth\")\n",
    "torch.save(model_test.state_dict(), f=modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/q2s37ndx6tg3lpzp3vp8xk_r0000gn/T/ipykernel_12150/1116700186.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelLoaded.load_state_dict(torch.load(modelPath))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLoaded = LinearRegression()\n",
    "modelLoaded.load_state_dict(torch.load(modelPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52.8610],\n",
       "        [52.8616],\n",
       "        [52.8622],\n",
       "        ...,\n",
       "        [62.0815],\n",
       "        [62.0821],\n",
       "        [62.0827]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLoaded.eval()\n",
    "with torch.inference_mode():\n",
    "    ypred = modelLoaded(test_X)\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss is 5.920702934265137 and test loss is 24.720218658447266, at epoch 0\n",
      "Train Loss is 11.994796752929688 and test loss is 14.601607322692871, at epoch 1\n",
      "Train Loss is 6.171483039855957 and test loss is 24.207609176635742, at epoch 2\n",
      "Train Loss is 11.758404731750488 and test loss is 15.114217758178711, at epoch 3\n",
      "Train Loss is 6.404669284820557 and test loss is 23.727916717529297, at epoch 4\n",
      "Train Loss is 11.537161827087402 and test loss is 15.593913078308105, at epoch 5\n",
      "Train Loss is 6.623079299926758 and test loss is 23.276409149169922, at epoch 6\n",
      "Train Loss is 11.328897476196289 and test loss is 16.0454158782959, at epoch 7\n",
      "Train Loss is 6.828814506530762 and test loss is 22.849233627319336, at epoch 8\n",
      "Train Loss is 11.131834983825684 and test loss is 16.47258949279785, at epoch 9\n",
      "Train Loss is 7.023600101470947 and test loss is 22.443466186523438, at epoch 10\n",
      "Train Loss is 10.94462776184082 and test loss is 16.87835693359375, at epoch 11\n",
      "Train Loss is 7.208737373352051 and test loss is 22.056615829467773, at epoch 12\n",
      "Train Loss is 10.766128540039062 and test loss is 17.26521110534668, at epoch 13\n",
      "Train Loss is 7.3853440284729 and test loss is 21.686574935913086, at epoch 14\n",
      "Train Loss is 10.595370292663574 and test loss is 17.63524627685547, at epoch 15\n",
      "Train Loss is 7.554360389709473 and test loss is 21.33173370361328, at epoch 16\n",
      "Train Loss is 10.431612968444824 and test loss is 17.99009132385254, at epoch 17\n",
      "Train Loss is 7.7165117263793945 and test loss is 20.99067497253418, at epoch 18\n",
      "Train Loss is 10.274197578430176 and test loss is 18.33115005493164, at epoch 19\n",
      "Train Loss is 7.872430324554443 and test loss is 20.662044525146484, at epoch 20\n",
      "Train Loss is 10.122511863708496 and test loss is 18.659780502319336, at epoch 21\n",
      "Train Loss is 8.022722244262695 and test loss is 20.344919204711914, at epoch 22\n",
      "Train Loss is 9.976118087768555 and test loss is 18.976905822753906, at epoch 23\n",
      "Train Loss is 8.167805671691895 and test loss is 20.038278579711914, at epoch 24\n",
      "Train Loss is 9.834558486938477 and test loss is 19.283544540405273, at epoch 25\n",
      "Train Loss is 8.308140754699707 and test loss is 19.741382598876953, at epoch 26\n",
      "Train Loss is 9.697484970092773 and test loss is 19.5804443359375, at epoch 27\n",
      "Train Loss is 8.444062232971191 and test loss is 19.453506469726562, at epoch 28\n",
      "Train Loss is 9.56456470489502 and test loss is 19.868318557739258, at epoch 29\n",
      "Train Loss is 8.575889587402344 and test loss is 19.173965454101562, at epoch 30\n",
      "Train Loss is 9.435482025146484 and test loss is 20.147859573364258, at epoch 31\n",
      "Train Loss is 8.703935623168945 and test loss is 18.902311325073242, at epoch 32\n",
      "Train Loss is 9.310035705566406 and test loss is 20.419517517089844, at epoch 33\n",
      "Train Loss is 8.828404426574707 and test loss is 18.637895584106445, at epoch 34\n",
      "Train Loss is 9.187922477722168 and test loss is 20.683937072753906, at epoch 35\n",
      "Train Loss is 8.9495849609375 and test loss is 18.380300521850586, at epoch 36\n",
      "Train Loss is 9.068952560424805 and test loss is 20.9415225982666, at epoch 37\n",
      "Train Loss is 9.067667007446289 and test loss is 18.129228591918945, at epoch 38\n",
      "Train Loss is 8.952986717224121 and test loss is 21.192596435546875, at epoch 39\n",
      "Train Loss is 9.182785034179688 and test loss is 17.884183883666992, at epoch 40\n",
      "Train Loss is 8.83979606628418 and test loss is 21.437639236450195, at epoch 41\n",
      "Train Loss is 9.29516315460205 and test loss is 17.644887924194336, at epoch 42\n",
      "Train Loss is 8.729254722595215 and test loss is 21.67693328857422, at epoch 43\n",
      "Train Loss is 9.404928207397461 and test loss is 17.41097068786621, at epoch 44\n",
      "Train Loss is 8.62119197845459 and test loss is 21.910858154296875, at epoch 45\n",
      "Train Loss is 9.512248039245605 and test loss is 17.182174682617188, at epoch 46\n",
      "Train Loss is 8.515486717224121 and test loss is 22.139652252197266, at epoch 47\n",
      "Train Loss is 9.617236137390137 and test loss is 16.958240509033203, at epoch 48\n",
      "Train Loss is 8.41202449798584 and test loss is 22.363588333129883, at epoch 49\n",
      "Train Loss is 9.720013618469238 and test loss is 16.738922119140625, at epoch 50\n",
      "Train Loss is 8.310687065124512 and test loss is 22.582904815673828, at epoch 51\n",
      "Train Loss is 9.820688247680664 and test loss is 16.523975372314453, at epoch 52\n",
      "Train Loss is 8.21136474609375 and test loss is 22.797853469848633, at epoch 53\n",
      "Train Loss is 9.919373512268066 and test loss is 16.313268661499023, at epoch 54\n",
      "Train Loss is 8.113995552062988 and test loss is 23.00855827331543, at epoch 55\n",
      "Train Loss is 10.016127586364746 and test loss is 16.10655975341797, at epoch 56\n",
      "Train Loss is 8.018468856811523 and test loss is 23.21526527404785, at epoch 57\n",
      "Train Loss is 10.111058235168457 and test loss is 15.903727531433105, at epoch 58\n",
      "Train Loss is 7.924729347229004 and test loss is 23.4180965423584, at epoch 59\n",
      "Train Loss is 10.204225540161133 and test loss is 15.704544067382812, at epoch 60\n",
      "Train Loss is 7.83267068862915 and test loss is 23.61728286743164, at epoch 61\n",
      "Train Loss is 10.295727729797363 and test loss is 15.508885383605957, at epoch 62\n",
      "Train Loss is 7.74223518371582 and test loss is 23.812942504882812, at epoch 63\n",
      "Train Loss is 10.385626792907715 and test loss is 15.316624641418457, at epoch 64\n",
      "Train Loss is 7.6533660888671875 and test loss is 24.005199432373047, at epoch 65\n",
      "Train Loss is 10.473976135253906 and test loss is 15.127547264099121, at epoch 66\n",
      "Train Loss is 7.565964698791504 and test loss is 24.194278717041016, at epoch 67\n",
      "Train Loss is 10.560870170593262 and test loss is 14.941636085510254, at epoch 68\n",
      "Train Loss is 7.480021953582764 and test loss is 24.380191802978516, at epoch 69\n",
      "Train Loss is 10.646323204040527 and test loss is 14.758681297302246, at epoch 70\n",
      "Train Loss is 7.395441055297852 and test loss is 24.56315040588379, at epoch 71\n",
      "Train Loss is 10.730428695678711 and test loss is 14.578563690185547, at epoch 72\n",
      "Train Loss is 7.312170505523682 and test loss is 24.743257522583008, at epoch 73\n",
      "Train Loss is 10.81323528289795 and test loss is 14.401274681091309, at epoch 74\n",
      "Train Loss is 7.230201244354248 and test loss is 24.920547485351562, at epoch 75\n",
      "Train Loss is 10.894752502441406 and test loss is 14.226699829101562, at epoch 76\n",
      "Train Loss is 7.149483680725098 and test loss is 25.095125198364258, at epoch 77\n",
      "Train Loss is 10.975034713745117 and test loss is 14.054642677307129, at epoch 78\n",
      "Train Loss is 7.069925785064697 and test loss is 25.267187118530273, at epoch 79\n",
      "Train Loss is 11.054167747497559 and test loss is 13.88508415222168, at epoch 80\n",
      "Train Loss is 6.99152135848999 and test loss is 25.436738967895508, at epoch 81\n",
      "Train Loss is 11.132152557373047 and test loss is 13.718008995056152, at epoch 82\n",
      "Train Loss is 6.9142608642578125 and test loss is 25.603822708129883, at epoch 83\n",
      "Train Loss is 11.209012031555176 and test loss is 13.553232192993164, at epoch 84\n",
      "Train Loss is 6.838058948516846 and test loss is 25.768592834472656, at epoch 85\n",
      "Train Loss is 11.284818649291992 and test loss is 13.390732765197754, at epoch 86\n",
      "Train Loss is 6.762906551361084 and test loss is 25.93109130859375, at epoch 87\n",
      "Train Loss is 11.359582901000977 and test loss is 13.2304105758667, at epoch 88\n",
      "Train Loss is 6.6887593269348145 and test loss is 26.091415405273438, at epoch 89\n",
      "Train Loss is 11.433356285095215 and test loss is 13.072171211242676, at epoch 90\n",
      "Train Loss is 6.615571975708008 and test loss is 26.249658584594727, at epoch 91\n",
      "Train Loss is 11.50617790222168 and test loss is 12.915996551513672, at epoch 92\n",
      "Train Loss is 6.543335437774658 and test loss is 26.405826568603516, at epoch 93\n",
      "Train Loss is 11.57805347442627 and test loss is 12.761878967285156, at epoch 94\n",
      "Train Loss is 6.472047805786133 and test loss is 26.55994987487793, at epoch 95\n",
      "Train Loss is 11.648991584777832 and test loss is 12.609639167785645, at epoch 96\n",
      "Train Loss is 6.401625633239746 and test loss is 26.712188720703125, at epoch 97\n",
      "Train Loss is 11.719072341918945 and test loss is 12.459344863891602, at epoch 98\n",
      "Train Loss is 6.332100868225098 and test loss is 26.862478256225586, at epoch 99\n",
      "Train Loss is 11.78825855255127 and test loss is 12.31082820892334, at epoch 100\n",
      "Train Loss is 6.2633957862854 and test loss is 27.011003494262695, at epoch 101\n",
      "Train Loss is 11.856640815734863 and test loss is 12.164069175720215, at epoch 102\n",
      "Train Loss is 6.195499897003174 and test loss is 27.157760620117188, at epoch 103\n",
      "Train Loss is 11.924211502075195 and test loss is 12.01906681060791, at epoch 104\n",
      "Train Loss is 6.128414630889893 and test loss is 27.30275535583496, at epoch 105\n",
      "Train Loss is 11.990981101989746 and test loss is 11.87572956085205, at epoch 106\n",
      "Train Loss is 6.062098026275635 and test loss is 27.44610023498535, at epoch 107\n",
      "Train Loss is 12.056991577148438 and test loss is 11.734044075012207, at epoch 108\n",
      "Train Loss is 5.996541500091553 and test loss is 27.587787628173828, at epoch 109\n",
      "Train Loss is 12.122246742248535 and test loss is 11.594005584716797, at epoch 110\n",
      "Train Loss is 5.93174409866333 and test loss is 27.72782325744629, at epoch 111\n",
      "Train Loss is 12.186746597290039 and test loss is 11.455520629882812, at epoch 112\n",
      "Train Loss is 5.8676652908325195 and test loss is 27.866308212280273, at epoch 113\n",
      "Train Loss is 12.250537872314453 and test loss is 11.318511009216309, at epoch 114\n",
      "Train Loss is 5.804264545440674 and test loss is 28.003314971923828, at epoch 115\n",
      "Train Loss is 12.313651084899902 and test loss is 11.182962417602539, at epoch 116\n",
      "Train Loss is 5.7415385246276855 and test loss is 28.13886070251465, at epoch 117\n",
      "Train Loss is 12.376096725463867 and test loss is 11.048872947692871, at epoch 118\n",
      "Train Loss is 5.6794843673706055 and test loss is 28.2729549407959, at epoch 119\n",
      "Train Loss is 12.43787956237793 and test loss is 10.916221618652344, at epoch 120\n",
      "Train Loss is 5.618095874786377 and test loss is 28.405593872070312, at epoch 121\n",
      "Train Loss is 12.498995780944824 and test loss is 10.784948348999023, at epoch 122\n",
      "Train Loss is 5.557339668273926 and test loss is 28.536876678466797, at epoch 123\n",
      "Train Loss is 12.559489250183105 and test loss is 10.654983520507812, at epoch 124\n",
      "Train Loss is 5.497189044952393 and test loss is 28.66684341430664, at epoch 125\n",
      "Train Loss is 12.61938190460205 and test loss is 10.526336669921875, at epoch 126\n",
      "Train Loss is 5.437645435333252 and test loss is 28.795490264892578, at epoch 127\n",
      "Train Loss is 12.678671836853027 and test loss is 10.398958206176758, at epoch 128\n",
      "Train Loss is 5.378686428070068 and test loss is 28.922866821289062, at epoch 129\n",
      "Train Loss is 12.73737907409668 and test loss is 10.272841453552246, at epoch 130\n",
      "Train Loss is 5.320309638977051 and test loss is 29.048982620239258, at epoch 131\n",
      "Train Loss is 12.795507431030273 and test loss is 10.147984504699707, at epoch 132\n",
      "Train Loss is 5.262514114379883 and test loss is 29.173837661743164, at epoch 133\n",
      "Train Loss is 12.85306167602539 and test loss is 10.024306297302246, at epoch 134\n",
      "Train Loss is 5.20526123046875 and test loss is 29.297521591186523, at epoch 135\n",
      "Train Loss is 12.910076141357422 and test loss is 9.901792526245117, at epoch 136\n",
      "Train Loss is 5.1485466957092285 and test loss is 29.420026779174805, at epoch 137\n",
      "Train Loss is 12.96655559539795 and test loss is 9.780448913574219, at epoch 138\n",
      "Train Loss is 5.092371463775635 and test loss is 29.54137420654297, at epoch 139\n",
      "Train Loss is 13.022502899169922 and test loss is 9.66019058227539, at epoch 140\n",
      "Train Loss is 5.0366950035095215 and test loss is 29.661636352539062, at epoch 141\n",
      "Train Loss is 13.077953338623047 and test loss is 9.54101276397705, at epoch 142\n",
      "Train Loss is 4.981518268585205 and test loss is 29.78081703186035, at epoch 143\n",
      "Train Loss is 13.132904052734375 and test loss is 9.42291259765625, at epoch 144\n",
      "Train Loss is 4.926839828491211 and test loss is 29.898910522460938, at epoch 145\n",
      "Train Loss is 13.187363624572754 and test loss is 9.305889129638672, at epoch 146\n",
      "Train Loss is 4.87265682220459 and test loss is 30.01593780517578, at epoch 147\n",
      "Train Loss is 13.241332054138184 and test loss is 9.189860343933105, at epoch 148\n",
      "Train Loss is 4.81893253326416 and test loss is 30.131961822509766, at epoch 149\n",
      "Train Loss is 13.294840812683105 and test loss is 9.074898719787598, at epoch 150\n",
      "Train Loss is 4.765699863433838 and test loss is 30.246925354003906, at epoch 151\n",
      "Train Loss is 13.34786319732666 and test loss is 8.960921287536621, at epoch 152\n",
      "Train Loss is 4.712922096252441 and test loss is 30.36090660095215, at epoch 153\n",
      "Train Loss is 13.400435447692871 and test loss is 8.847929954528809, at epoch 154\n",
      "Train Loss is 4.660599231719971 and test loss is 30.47389793395996, at epoch 155\n",
      "Train Loss is 13.452555656433105 and test loss is 8.735852241516113, at epoch 156\n",
      "Train Loss is 4.608696937561035 and test loss is 30.585969924926758, at epoch 157\n",
      "Train Loss is 13.504257202148438 and test loss is 8.624743461608887, at epoch 158\n",
      "Train Loss is 4.557243347167969 and test loss is 30.697080612182617, at epoch 159\n",
      "Train Loss is 13.555511474609375 and test loss is 8.514542579650879, at epoch 160\n",
      "Train Loss is 4.506206512451172 and test loss is 30.807283401489258, at epoch 161\n",
      "Train Loss is 13.606352806091309 and test loss is 8.405240058898926, at epoch 162\n",
      "Train Loss is 4.455585479736328 and test loss is 30.916587829589844, at epoch 163\n",
      "Train Loss is 13.656784057617188 and test loss is 8.296828269958496, at epoch 164\n",
      "Train Loss is 4.405375003814697 and test loss is 31.024999618530273, at epoch 165\n",
      "Train Loss is 13.706804275512695 and test loss is 8.189239501953125, at epoch 166\n",
      "Train Loss is 4.355543613433838 and test loss is 31.13258171081543, at epoch 167\n",
      "Train Loss is 13.756446838378906 and test loss is 8.082544326782227, at epoch 168\n",
      "Train Loss is 4.306125164031982 and test loss is 31.239280700683594, at epoch 169\n",
      "Train Loss is 13.805681228637695 and test loss is 7.976660251617432, at epoch 170\n",
      "Train Loss is 4.257080554962158 and test loss is 31.345169067382812, at epoch 171\n",
      "Train Loss is 13.854543685913086 and test loss is 7.871590614318848, at epoch 172\n",
      "Train Loss is 4.208411693572998 and test loss is 31.45023536682129, at epoch 173\n",
      "Train Loss is 13.903032302856445 and test loss is 7.767334938049316, at epoch 174\n",
      "Train Loss is 4.160118579864502 and test loss is 31.554487228393555, at epoch 175\n",
      "Train Loss is 13.951149940490723 and test loss is 7.66387939453125, at epoch 176\n",
      "Train Loss is 4.112195014953613 and test loss is 31.657943725585938, at epoch 177\n",
      "Train Loss is 13.998896598815918 and test loss is 7.561236381530762, at epoch 178\n",
      "Train Loss is 4.0646443367004395 and test loss is 31.76058578491211, at epoch 179\n",
      "Train Loss is 14.046276092529297 and test loss is 7.459384441375732, at epoch 180\n",
      "Train Loss is 4.017460346221924 and test loss is 31.862436294555664, at epoch 181\n",
      "Train Loss is 14.09328842163086 and test loss is 7.358264446258545, at epoch 182\n",
      "Train Loss is 3.970613956451416 and test loss is 31.963558197021484, at epoch 183\n",
      "Train Loss is 14.139966011047363 and test loss is 7.257874965667725, at epoch 184\n",
      "Train Loss is 3.924104690551758 and test loss is 32.0639533996582, at epoch 185\n",
      "Train Loss is 14.186310768127441 and test loss is 7.158210277557373, at epoch 186\n",
      "Train Loss is 3.8779296875 and test loss is 32.16361618041992, at epoch 187\n",
      "Train Loss is 14.232320785522461 and test loss is 7.059272766113281, at epoch 188\n",
      "Train Loss is 3.83208966255188 and test loss is 32.262550354003906, at epoch 189\n",
      "Train Loss is 14.277997016906738 and test loss is 6.961050987243652, at epoch 190\n",
      "Train Loss is 3.786580801010132 and test loss is 32.360774993896484, at epoch 191\n",
      "Train Loss is 14.323347091674805 and test loss is 6.863551139831543, at epoch 192\n",
      "Train Loss is 3.741403579711914 and test loss is 32.458274841308594, at epoch 193\n",
      "Train Loss is 14.368366241455078 and test loss is 6.76669979095459, at epoch 194\n",
      "Train Loss is 3.696526527404785 and test loss is 32.55512237548828, at epoch 195\n",
      "Train Loss is 14.413084983825684 and test loss is 6.670558929443359, at epoch 196\n",
      "Train Loss is 3.651978015899658 and test loss is 32.651268005371094, at epoch 197\n",
      "Train Loss is 14.457480430603027 and test loss is 6.575066566467285, at epoch 198\n",
      "Train Loss is 3.607727289199829 and test loss is 32.74675750732422, at epoch 199\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "class LinearRegressionModeV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "modelV2 = LinearRegressionModeV2()\n",
    "lossFn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=modelV2.parameters(), lr=0.01)\n",
    "epochs = 200\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    modelV2.train()\n",
    "    y_pred = modelV2(train_X)\n",
    "\n",
    "    loss = lossFn(y_pred, train_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    modelV2.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = modelV2(test_X)\n",
    "        losstest = lossFn(test_pred, test_Y)\n",
    "    \n",
    "    print(f\"Train Loss is {loss} and test loss is {losstest}, at epoch {each_epoch}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[0.9656]])),\n",
       "             ('linear.bias', tensor([0.9245]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelV2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
